---
title: Do Large Language Models Believe?

event: Tübingen Linguistics Department
#event_url: https://example.org

location: University of Tübingen, Linguistics Department
# address:
#  street: 450 Serra Mall
#  city: Stanford
 # region: CA
 # postcode: '94305'
 # country: United States

summary: Some normative properties of belief are not instantiated in the belief-like states of LLMs.
abstract: 'Here is an increasingly popular view: if believing that P is just to be disposed to assert, infer, and act as if you take P to be true, then large language models (LLMs) have beliefs. On this view, LLMs pass the relevant behavioral tests: they assert truths, pass theory-of-mind evaluations, and exhibit a degree of internal consistency. This talk pushes back. I show that even if we accept the dispositionalist conception of belief, current LLMs do not meet relevant standards for an important kind of belief ascription. I develop three arguments for this claim.

First, the coherence threshold: belief attribution presupposes some degree of internal consistency. But LLMs are so prompt-sensitive that their belief-like assertions are subject to radical incoherence even within one context window. There is no practice-independent fact about how much logical coherence belief-ascription requires. So we are free to demand a level of coherence high enough to support explanation and prediction. Given the volatility of LLM assertion, we can easily choose to impose a coherence threshold that they cannot yet meet.

Second, the proportionality constraint: in humans and non-human animals, belief ascription is only predictively useful when paired with a model of desire. In LLMs, inconsistency carries no cost, because nothing is at stake. Since nothing depends on which belief-like attitudes the system adopts, nothing stabilizes its content-bearing mechanisms over time.

Third, the mindshaping argument: in humans, attitudes are aligned—imperfectly but meaningfully—with cognitive mechanisms, due to social pressures that make us accountable for what we say and do. This alignment makes belief-ascription explanatory rather than merely predictive. In LLMs, this alignment mechanism is absent. Their norm-sensitive belief-like assertions are not anchored to underlying cognitive mechanisms at inference time. Consequently, belief-ascription risks tracking surface regularities that fail to generalize.

These three arguments show that belief-ascription to LLMs faces philosophical obstacles. I wrap up by noting that the situation may look different for the emerging class of AI agents — systems with persistent memory, planning capacity, and goal-conditioned behavior. '

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2025-12-08T13:00:00Z'
# date_end: '2030-06-01T15:00:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2017-01-01T00:00:00Z'

authors:
  - admin

tags: []

# Is this a featured talk? (true/false)
featured: true

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
  focal_point: Right

#links:
#  - icon: twitter
#    icon_pack: fab
#    name: Follow
#    url: https://twitter.com/georgecushen
# url_code: 'https://github.com'
# url_pdf: ''
# url_slides: 'https://slideshare.net'
# url_video: 'https://youtube.com'

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
  - example
---

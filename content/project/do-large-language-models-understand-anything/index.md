---
title: LLM Cognition and Evaluation
date: 2022-10-07T17:54:46.497Z
draft: false
featured: true
share: false
summary: How should we think about and evaluate the cognitive capacities of large language models?
abstract: When confronted with demonstrations of ChatGPT, the reaction is often either that machines now understand language, or that they are nothing but stochastic parrots. The truth is more complicated. We need frameworks for thinking about LLM cognition that avoid both anthropomorphism and anthropocentrism, and evaluation methods that are sensitive to how LLMs differ from humans in architecturally relevant ways.  

slides: example
---

Do large language models have beliefs? Can they understand? What would it mean for them to be reliable despite hallucination? These questions require us to develop new conceptual frameworks that take seriously both the achievements and limitations of current AI systems.

My work in this area focuses on two interconnected themes: (1) developing philosophically grounded accounts of LLM cognition that avoid treating them as either human-like minds or mere statistical tools, and (2) identifying biases in how we evaluate LLMs that stem from importing assumptions from human psychology without justification.

## Associated publications

{{<cite page="/publication/complingeval" view="3" >}}
{{<cite page="/publication/icml" view="3" >}}
{{<cite page="/publication/all_or_nothing" view="3" >}}
{{<cite page="/publication/bigbench" view="3" >}}
